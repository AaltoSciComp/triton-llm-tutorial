{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load env variables, NOTE: this cell must be run first\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(override=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_OFFLINE']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/shareddata/dldata/huggingface-hub-cache/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|███████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2492.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.39s/it]\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-2-7b-hf', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-2-7b-hf', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "Downloading shards: 100%|███████████████████████████████████████████████████████| 2/2 [00:00<00:00, 3007.75it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.22s/it]\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-2-7b-hf', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-2-7b-hf', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# policy network\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name,\n",
    "                                                         load_in_8bit=True,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                 device_map=\"cuda:0\"\n",
    "                                                         )\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_name,\n",
    "                                                         load_in_8bit=True,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                 device_map=\"cuda:1\"\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_params: 262414337\n",
      "all_params: 6738419713\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the names and shapes of trainable parameters in a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "    model: A Hugging Face model instance.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable_params: {trainable_params}\")\n",
    "    print(f\"all_params: {all_params}\")\n",
    "    \n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Print out the layers, data types, and devices\n",
    "\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     for param_name, param in module.named_parameters(recurse=False):\n",
    "#         print(f\"Layer: {name}, Parameter: {param_name}, Type: {param.dtype}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=4096, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "ppo_config = {\"batch_size\":1}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(config,model,model_ref,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pretrained_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Are you happy\"  # Define your query\n",
    "\n",
    "inputs = tokenizer.encode(query, return_tensors=\"pt\").to(model.pretrained_model.device)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are you happy with your current salary?\\nDo you feel like you are being paid what you are worth?\\nI am sure you have heard the saying, “If you are not getting paid what you are worth, you are worth what'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_kwargs = {\"min_length\": -1, \n",
    "                     \"top_k\": 0.0, \n",
    "                     \"top_p\": 1.0, \n",
    "                     \"do_sample\": True, \n",
    "                     \"pad_token_id\": tokenizer.eos_token_id\n",
    "                    }\n",
    "outputs = ppo_trainer.generate([item for item in inputs],return_prompt= True, max_length=50)\n",
    "respond = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['How do you like my new hair cut?',\n",
    "           'Do you like Tayler Swift?'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do you like my new hair cut?\\nI got it from the barber shop in the mall.\\nThe one with the big mirrors.\\nI like it, but I think you should wear it with a hat.\\n',\n",
       " \"Do you like Tayler Swift?\\nI don't like her but I like her songs.\\nI don't like her but I like her songs. I like her songs, but I don't like her.\\n\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(prompts, padding=True,\n",
    "                   truncation=True,\n",
    "                   max_length=30, \n",
    "                   return_tensors=\"pt\"\n",
    "                  ).to(model.pretrained_model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Decode the outputs\n",
    "responds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "responds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.5353606343269348},\n",
       " {'label': 'NEU', 'score': 0.838121235370636}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier(responds)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5353606343269348, -0.838121235370636]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = []\n",
    "for result in results:\n",
    "    if result['label']=='POS':\n",
    "        reward = result['score']\n",
    "    else: reward = -result['score']\n",
    "    rewards.append(reward)\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = tokenizer(prompts, padding=True, truncation=True,\\\n",
    "                   max_length=30, return_tensors=\"pt\")['input_ids'].to(model.pretrained_model.device)\n",
    "input_tensors = []\n",
    "for prompt in prompts:\n",
    "    input_ = tokenizer(prompt, padding=True, truncation=True,\\\n",
    "                   max_length=30, return_tensors=\"pt\")['input_ids']\n",
    "    input_tensors.append(input_.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get response \n",
    "response_tensors = []\n",
    "for input_tensor in input_tensors:\n",
    "    response = ppo_trainer.generate(input_tensor, **generation_kwargs)\n",
    "    response_tensors.append(response.squeeze())\n",
    "response_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return response\n",
    "    outputs = model.generate(**inputs)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"Are you happy\"  # Define your query\n",
    "response = generate_response(query)\n",
    "reward = get_reward(response)\n",
    "\n",
    "# Step the trainer\n",
    "ppo_trainer.step(reward)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classify_sequences(examples, threshold=0.5):\n",
    "\n",
    "    # first, send batch of texts through pipeline\n",
    "    texts = examples['text']\n",
    "    outputs = classifier(texts, candidate_labels, multi_label=True)\n",
    "    # next, for each output:\n",
    "    final_outputs = []\n",
    "    for output in outputs:\n",
    "        # create dictionary (predicted_labels, confidence)\n",
    "        final_output = {}\n",
    "        for label, score in zip(output['labels'], output['scores']):\n",
    "            if score > threshold:\n",
    "               final_output[label] = score\n",
    "        final_outputs.append(final_output)\n",
    "\n",
    "    assert len(final_outputs) == len(texts)\n",
    "    # set final outputs\n",
    "    examples['predicted_labels'] = final_outputs\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = dataset.map(zero_shot_classify_sequences, batched=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel2",
   "language": "python",
   "name": "kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
